{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"toc":{"nav_menu":{"height":"263px","width":"352px"},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"name":"summarization.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"rXLMzokXds8T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"06f2f33b-a9b4-40a3-a4a9-2a44fb473026"},"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf \n","import numpy as np\n","import nltk\n","nltk.download(\"stopwords\")\n","from nltk.corpus import stopwords #provides list of english stopwords\n","stop = stopwords.words('english')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HeAT6X7YdzXH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":130},"outputId":"7c41dee2-bdbd-447b-ee88-5276d4b4edad"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"l0CSt_ZMds8l","colab_type":"code","colab":{}},"source":["train = pd.read_csv('/content/drive/My Drive/NLP Project/reviews.csv')#,  nrows=1000)  #, nrows=100000 sep='\\t',"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7CQ3eKHyds8r","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":293},"outputId":"dd226db8-7b9e-431b-c055-685f07e0b4b6"},"source":["train.head() "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>ProductId</th>\n","      <th>UserId</th>\n","      <th>ProfileName</th>\n","      <th>HelpfulnessNumerator</th>\n","      <th>HelpfulnessDenominator</th>\n","      <th>Score</th>\n","      <th>Time</th>\n","      <th>Summary</th>\n","      <th>Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>B001E4KFG0</td>\n","      <td>A3SGXH7AUHU8GW</td>\n","      <td>delmartian</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>1303862400</td>\n","      <td>Good Quality Dog Food</td>\n","      <td>I have bought several of the Vitality canned d...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>B00813GRG4</td>\n","      <td>A1D87F6ZCVE5NK</td>\n","      <td>dll pa</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1346976000</td>\n","      <td>Not as Advertised</td>\n","      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>B000LQOCH0</td>\n","      <td>ABXLMWJIXXAIN</td>\n","      <td>Natalia Corres \"Natalia Corres\"</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1219017600</td>\n","      <td>\"Delight\" says it all</td>\n","      <td>This is a confection that has been around a fe...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>B000UA0QIQ</td>\n","      <td>A395BORC6FGVXV</td>\n","      <td>Karl</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>1307923200</td>\n","      <td>Cough Medicine</td>\n","      <td>If you are looking for the secret ingredient i...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>B006K2ZZ7K</td>\n","      <td>A1UQRSCLF8GW1T</td>\n","      <td>Michael D. Bigham \"M. Wassir\"</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>1350777600</td>\n","      <td>Great taffy</td>\n","      <td>Great taffy at a great price.  There was a wid...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Id  ...                                               Text\n","0   1  ...  I have bought several of the Vitality canned d...\n","1   2  ...  Product arrived labeled as Jumbo Salted Peanut...\n","2   3  ...  This is a confection that has been around a fe...\n","3   4  ...  If you are looking for the secret ingredient i...\n","4   5  ...  Great taffy at a great price.  There was a wid...\n","\n","[5 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"ji-HIidnds8x","colab_type":"code","colab":{}},"source":["train = train[['Summary','Text']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rw1iSK-lds9T","colab_type":"code","colab":{}},"source":["train['text_lower'] = train['Text'].str.lower()\n","train['text_no_punctuation'] = train['text_lower'].str.replace('[^\\w\\s]','')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UQS1cq2tds9X","colab_type":"code","colab":{}},"source":["train['summary_lower'] = train[\"Summary\"].str.lower()\n","train['summary_no_punctuation'] =  '_start_' + ' ' +train['summary_lower'].str.replace('[^\\w\\s]','')+ ' ' +'_end_'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"6VvLtdruds9b","colab_type":"text"},"source":["**VERY IMPORTANT TRICK!! NOTICE THAT WE ADD \"_start_\" and \"_end_\" EXACTLY AT THE BEGINNING AND THE END OF EACH SENTENCE TO HAVE SOME KIND OF'DELIMITERS' THAT WILL TELL OUR DECODER TO START AND FINISH. BECAUSE WE DON'T HAVE GENERAL SIGNALS OF START AND FINISH IN NATURAL LANGUAGE. BASICALLY '_end_' REFLECTS THE POINT IN WHICH OUR OUTPUT SENTENCE IS MORE LIKELY TO END.**"]},{"cell_type":"code","metadata":{"id":"jlyBo906ds9x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":424},"outputId":"5445d84e-ea41-462b-fe88-13941fc8a16a"},"source":["train = train.drop(columns=['Summary','Text','text_lower','summary_lower'])\n","train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text_no_punctuation</th>\n","      <th>summary_no_punctuation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>i have bought several of the vitality canned d...</td>\n","      <td>_start_ good quality dog food _end_</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>product arrived labeled as jumbo salted peanut...</td>\n","      <td>_start_ not as advertised _end_</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>this is a confection that has been around a fe...</td>\n","      <td>_start_ delight says it all _end_</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>if you are looking for the secret ingredient i...</td>\n","      <td>_start_ cough medicine _end_</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>great taffy at a great price  there was a wide...</td>\n","      <td>_start_ great taffy _end_</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>568449</th>\n","      <td>great for sesame chickenthis is a good if not ...</td>\n","      <td>_start_ will not do without _end_</td>\n","    </tr>\n","    <tr>\n","      <th>568450</th>\n","      <td>im disappointed with the flavor the chocolate ...</td>\n","      <td>_start_ disappointed _end_</td>\n","    </tr>\n","    <tr>\n","      <th>568451</th>\n","      <td>these stars are small so you can give 1015 of ...</td>\n","      <td>_start_ perfect for our maltipoo _end_</td>\n","    </tr>\n","    <tr>\n","      <th>568452</th>\n","      <td>these are the best treats for training and rew...</td>\n","      <td>_start_ favorite training and reward treat _end_</td>\n","    </tr>\n","    <tr>\n","      <th>568453</th>\n","      <td>i am very satisfied product is as advertised i...</td>\n","      <td>_start_ great honey _end_</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>568454 rows × 2 columns</p>\n","</div>"],"text/plain":["                                      text_no_punctuation                            summary_no_punctuation\n","0       i have bought several of the vitality canned d...               _start_ good quality dog food _end_\n","1       product arrived labeled as jumbo salted peanut...                   _start_ not as advertised _end_\n","2       this is a confection that has been around a fe...                 _start_ delight says it all _end_\n","3       if you are looking for the secret ingredient i...                      _start_ cough medicine _end_\n","4       great taffy at a great price  there was a wide...                         _start_ great taffy _end_\n","...                                                   ...                                               ...\n","568449  great for sesame chickenthis is a good if not ...                 _start_ will not do without _end_\n","568450  im disappointed with the flavor the chocolate ...                        _start_ disappointed _end_\n","568451  these stars are small so you can give 1015 of ...            _start_ perfect for our maltipoo _end_\n","568452  these are the best treats for training and rew...  _start_ favorite training and reward treat _end_\n","568453  i am very satisfied product is as advertised i...                         _start_ great honey _end_\n","\n","[568454 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"zk5KcmvDds9g","colab_type":"code","colab":{}},"source":["max_features1 = 5000\n","max_features2 = 5000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HysEdlAeds9k","colab_type":"code","colab":{}},"source":["tok1 = tf.keras.preprocessing.text.Tokenizer(num_words=max_features1) \n","tok1.fit_on_texts(list(train['text_no_punctuation'].astype(str)))\n","tf_train_text =tok1.texts_to_sequences(list(train['text_no_punctuation'].astype(str)))\n","tf_train_text =tf.keras.preprocessing.sequence.pad_sequences(tf_train_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iMRQJURjds9v","colab_type":"code","colab":{}},"source":["tok2 = tf.keras.preprocessing.text.Tokenizer(num_words=max_features2, filters = '*') \n","tok2.fit_on_texts(list(train['summary_no_punctuation'].astype(str)))\n","tf_train_summary = tok2.texts_to_sequences(list(train['summary_no_punctuation'].astype(str)))\n","tf_train_summary = tf.keras.preprocessing.sequence.pad_sequences(tf_train_summary, padding ='post')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DMFFuoxads91","colab_type":"text"},"source":["# Define Model Architecture"]},{"cell_type":"code","metadata":{"id":"wwUOS7RCds91","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"ae4a50af-cd70-4a10-f602-c3592f613714"},"source":["vectorized_summary = tf_train_summary\n","decoder_input_data = vectorized_summary[:, :-1]\n","\n","decoder_target_data = vectorized_summary[:, 1:]\n","vectorized_text = tf_train_text\n","encoder_input_data = vectorized_text\n","doc_length = encoder_input_data.shape[1]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Shape of decoder input: (568454, 39)\n","Shape of decoder target: (568454, 39)\n","Shape of encoder input: (568454, 2961)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Siwa8i6Qds97","colab_type":"code","colab":{}},"source":["vocab_size_encoder = len(tok1.word_index) + 1\n","vocab_size_decoder = len(tok2.word_index) + 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z-ZMrt4Pds-D","colab_type":"text"},"source":["### Define Model Architecture"]},{"cell_type":"code","metadata":{"id":"M_TBtmwzds-D","colab_type":"code","colab":{}},"source":["#arbitrarly set latent dimension for embedding and hidden units\n","#We previously represented word with array of numbers. But it means nothing to our model. \n","#We should represent each word in a vector representation such that we can understand its meaning by finding its word embedding.\n","#So we need to learn this representation that can give word embeeddings for each word. \n","#This latent dimension is what your word embedding look like means each word is represented as a 300 dimensional vector\n","latent_dim = 300\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aUhjq8FYds-H","colab_type":"code","colab":{}},"source":["encoder_inputs = tf.keras.Input(shape=(doc_length,), name='Encoder-Input')\n","\n","# Word embeding for encoder (English text)\n","x = tf.keras.layers.Embedding(vocab_size_encoder, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n","\n","\n","#Batch normalization is used so that the distribution of the inputs \n","#to a specific layer doesn't change over time\n","x = tf.keras.layers.BatchNormalization(name='Encoder-Batchnorm-1')(x)\n","\n","\n","# We do not need the `encoder_output` just the hidden state.\n","_, state_h = tf.keras.layers.GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n","\n","# Encapsulate the encoder as a separate entity so we can just \n","#  encode without decoding if we want to.\n","encoder_model = tf.keras.Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n","\n","seq2seq_encoder_out = encoder_model(encoder_inputs)\n","\n","#### Decoder Model ####\n","decoder_inputs = tf.keras.Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n","\n","# Word Embedding For Decoder\n","dec_emb = tf.keras.layers.Embedding(vocab_size_decoder, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n","#again batch normalization\n","dec_bn = tf.keras.layers.BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n","\n","# Set up the decoder, using `decoder_state_input` as initial state.\n","decoder_gru = tf.keras.layers.GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n","decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out) #the decoder \"decodes\" the encoder output.\n","x = tf.keras.layers.BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n","\n","# Dense layer for prediction\n","decoder_dense = tf.keras.layers.Dense(vocab_size_decoder, activation='softmax', name='Final-Output-Dense')\n","decoder_outputs = decoder_dense(x)\n","\n","#### Seq2Seq Model ####\n","seq2seq_Model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","seq2seq_Model.compile(optimizer=tf.keras.optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uZy7AesIds-K","colab_type":"text"},"source":["** Examine Model Architecture Summary **"]},{"cell_type":"code","metadata":{"id":"BgAeAHaeds-K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":481},"outputId":"dd978368-7a96-4672-9db5-d310b3f93bda"},"source":["seq2seq_Model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","Decoder-Input (InputLayer)      [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","Decoder-Word-Embedding (Embeddi (None, None, 300)    12858600    Decoder-Input[0][0]              \n","__________________________________________________________________________________________________\n","Encoder-Input (InputLayer)      [(None, 2961)]       0                                            \n","__________________________________________________________________________________________________\n","Decoder-Batchnorm-1 (BatchNorma (None, None, 300)    1200        Decoder-Word-Embedding[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-Model (Model)           (None, 300)          72705000    Encoder-Input[0][0]              \n","__________________________________________________________________________________________________\n","Decoder-GRU (GRU)               [(None, None, 300),  541800      Decoder-Batchnorm-1[0][0]        \n","                                                                 Encoder-Model[1][0]              \n","__________________________________________________________________________________________________\n","Decoder-Batchnorm-2 (BatchNorma (None, None, 300)    1200        Decoder-GRU[0][0]                \n","__________________________________________________________________________________________________\n","Final-Output-Dense (Dense)      (None, None, 42862)  12901462    Decoder-Batchnorm-2[0][0]        \n","==================================================================================================\n","Total params: 99,009,262\n","Trainable params: 99,007,462\n","Non-trainable params: 1,800\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ly4q7IVZds-P","colab_type":"text"},"source":["# Train Model"]},{"cell_type":"code","metadata":{"id":"6UBkeE1Hds-Q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":56},"outputId":"01ec1b9c-4200-4533-a460-dfae969a2d56"},"source":["'''\n","batch_size = 64\n","epochs = 3 \n","history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n","          batch_size=batch_size,  epochs=epochs ,  validation_split=0.12) \n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nbatch_size = 64\\nepochs = 3 \\nhistory = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\\n          batch_size=batch_size,  epochs=epochs ,  validation_split=0.12) \\n'"]},"metadata":{"tags":[]},"execution_count":182}]},{"cell_type":"code","metadata":{"id":"K-sri95jrBhF","colab_type":"code","colab":{}},"source":["seq2seq_Model.load_weights('drive/My Drive/Colab Notebooks/1-model/final_model_weights.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4RwhWnL8ds-X","colab_type":"code","colab":{}},"source":["sam=\"Give your Sample text here to summarize it\"\n","test_text = [sam]\n","tok1.fit_on_texts(test_text)\n","raw_tokenized = tok1.texts_to_sequences(test_text)\n","raw_tokenized = tf.keras.preprocessing.sequence.pad_sequences(raw_tokenized, maxlen=len(tf_train_text[0]))\n","body_encoding = encoder_model.predict(raw_tokenized)\n","latent_dim = seq2seq_Model.get_layer('Decoder-Word-Embedding').output_shape[-1]\n","decoder_inputs = seq2seq_Model.get_layer('Decoder-Input').input \n","dec_emb = seq2seq_Model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n","dec_bn = seq2seq_Model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n","gru_inference_state_input = tf.keras.Input(shape=(latent_dim,), name='hidden_state_input')\n","gru_out, gru_state_out = seq2seq_Model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input])\n","dec_bn2 = seq2seq_Model.get_layer('Decoder-Batchnorm-2')(gru_out)\n","dense_out = seq2seq_Model.get_layer('Final-Output-Dense')(dec_bn2)\n","decoder_model = tf.keras.Model([decoder_inputs, gru_inference_state_input],\n","                      [dense_out, gru_state_out])\n","original_body_encoding = body_encoding\n","state_value = np.array(tok2.word_index['_start_']).reshape(1, 1)\n","decoded_sentence = []\n","stop_condition = False\n","vocabulary_inv = dict((v, k) for k, v in tok2.word_index.items())\n","while not stop_condition:\n","    preds, st = decoder_model.predict([state_value, body_encoding])\n","\n","    pred_idx = np.argmax(preds[:, :, 2:]) + 2\n","    pred_word_str = vocabulary_inv[pred_idx]\n","    if pred_word_str == '_end_':\n","        stop_condition = True\n","        break\n","    print(pred_word_str)\n","    decoded_sentence.append(pred_word_str)\n","    body_encoding = st\n","    state_value = np.array(pred_idx).reshape(1, 1)"],"execution_count":null,"outputs":[]}]}